import os
import requests
from datetime import datetime, timedelta
from arxiv import Client, Search, SortCriterion, SortOrder
from PyPDF2 import PdfReader
import openai

from config import AI_CONFIG, EMAIL_SERVER_CONFIG, GENERAL_CONFIG, USERS_CONFIG, DEFAULT_PROMPT_TEMPLATE
from database import get_db

import smtplib
import socket
import asyncio
from email.mime.text import MIMEText
import markdown2  # å¯¼å…¥markdown2åº“
from loguru import logger
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
import time
from bs4 import BeautifulSoup
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed



async def send_email(subject, content, receiver_email):
    """å‘é€é‚®ä»¶é€šçŸ¥ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
    # å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML
    html_content = markdown2.markdown(content, extras=["tables", "latex", "fenced-code-blocks"])
    msg = MIMEText(html_content, "html", "utf-8")
    msg["Subject"] = subject
    msg["From"] = EMAIL_SERVER_CONFIG["sender"]
    msg["To"] = receiver_email

    server = None
    try:
        logger.info(f"æ­£åœ¨è¿æ¥SMTPæœåŠ¡å™¨ï¼Œå‘é€ç»™ {receiver_email}...")
        # å°†SMTPæ“ä½œæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        return await asyncio.get_event_loop().run_in_executor(
            None, lambda: _send_email_sync(msg, server, receiver_email)
        )
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False


def _send_email_sync(msg, server=None, receiver_email=None):
    """åŒæ­¥å‘é€é‚®ä»¶çš„å†…éƒ¨å‡½æ•°"""
    try:
        server = smtplib.SMTP(
            EMAIL_SERVER_CONFIG["smtp_server"], EMAIL_SERVER_CONFIG["smtp_port"], timeout=10
        )
        server.starttls()  # å¯ç”¨TLSåŠ å¯†
        server.login(EMAIL_SERVER_CONFIG["sender"], EMAIL_SERVER_CONFIG["password"])

        if receiver_email.count(",") > 0:
            receivers = receiver_email.split(",")
            server.sendmail(EMAIL_SERVER_CONFIG["sender"], receivers, msg.as_string())
        else:
            server.sendmail(
                EMAIL_SERVER_CONFIG["sender"], [receiver_email], msg.as_string()
            )

        logger.success("é‚®ä»¶å‘é€æˆåŠŸ")
        return True
    except socket.timeout:
        logger.warning("è¿æ¥SMTPæœåŠ¡å™¨è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡é‚®ä»¶å‘é€")
        return False
    except smtplib.SMTPException as e:
        logger.error(
            f"SMTPé”™è¯¯: {e.smtp_error.decode() if hasattr(e, 'smtp_error') else str(e)}"
        )
        return False
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False
    finally:
        if server:
            try:
                server.quit()
            except Exception as e:
                logger.warning(f"å…³é—­SMTPè¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


def fetch_papers(arxiv_categories):
    """è·å–æŒ‡å®šåˆ†ç±»çš„è®ºæ–‡"""
    # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ŒåªåŒ…å«é…ç½®ä¸­çš„ä¸»é¢˜
    search_query = " OR ".join([f"cat:{cat}" for cat in arxiv_categories])
    client = Client(
        page_size=50,  # å‡å°æ¯é¡µå¤§å°
        delay_seconds=3,  # å¢åŠ è¯·æ±‚é—´éš”åˆ°3ç§’ï¼Œé¿å…è¢«é™æµ
        num_retries=5  # å¢åŠ é‡è¯•æ¬¡æ•°
    )
    search = Search(
        query=search_query,
        sort_by=SortCriterion.SubmittedDate,
        sort_order=SortOrder.Descending,
        max_results=100
    )

    papers = []
    # Get the target date (previous workday)
    today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    target_date = today - timedelta(days=GENERAL_CONFIG["days_lookback"])

    # Adjust if yesterday was a weekend
    weekday = target_date.weekday()  # 0-6, where 5 is Saturday and 6 is Sunday
    if weekday >= 5:  # If Saturday or Sunday
        # Go back to Friday (4)
        target_date -= timedelta(days=weekday - 4)

    logger.info(f"Target date set to previous workday: {target_date.strftime('%Y-%m-%d')}")
    for result in client.results(search):
        logger.info(f"Processing paper: {result.title} published on {result.published}")
        # Check if the paper was published on the target date
        published_dt = result.published.replace(tzinfo=None)
        if target_date <= published_dt :
            papers.append({
                "title": result.title,
                "url": result.entry_id,
                "pdf_url": result.pdf_url,
                "abstract": result.summary,
                "authors": [a.name for a in result.authors],
                "published": result.published,
                "categories": [c for c in result.categories],
                "primary_category": result.primary_category if result.primary_category else None
            })
    logger.success(f"Found {len(papers)} papers published from {target_date.strftime('%Y-%m-%d')}")
    return papers

def download_pdf(url, filename, max_retries=3):
    """ä¸‹è½½PDFæ–‡ä»¶ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""
        # ç¡®ä¿URLæ˜¯æ­£ç¡®çš„PDFé“¾æ¥
    if 'arxiv.org' in url and not url.endswith('.pdf'):
        # ä»URLæå–è®ºæ–‡ID
        paper_id = url.split('/')[-1]
        url = f"https://arxiv.org/pdf/{paper_id}.pdf"
    
    logger.info(f"å°è¯•ä¸‹è½½: {url}")
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)  # æ·»åŠ è¶…æ—¶å‚æ•°
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸä¸”å†…å®¹ç±»å‹æ˜¯PDF
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '')
                if 'pdf' not in content_type.lower() and len(response.content) < 10000:
                    logger.warning(f"å“åº”å¯èƒ½ä¸æ˜¯PDFæ–‡ä»¶ (Content-Type: {content_type})")
                
                with open(filename, 'wb') as f:
                    f.write(response.content)
                
                # éªŒè¯æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(filename)
                if file_size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
                    logger.warning(f"ä¸‹è½½çš„æ–‡ä»¶è¿‡å° ({file_size} å­—èŠ‚)")
                    continue
                
                return True
            else:
                logger.error(f"ä¸‹è½½å¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            logger.warning(f"å°è¯• {attempt+1}/{max_retries} å¤±è´¥: {str(e)}")
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
        if attempt < max_retries - 1:
            time.sleep(2 * (attempt + 1))  # æŒ‡æ•°é€€é¿
    
    return False

def extract_text_from_pdf(pdf_path, paper):
    """ä»PDFæå–æ–‡æœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    text = ""
    try:
        with open(pdf_path, 'rb') as f:
            try:
                reader = PdfReader(f)
                for page_num, page in enumerate(reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except Exception as e:
                        logger.warning(f"æ— æ³•æå–ç¬¬ {page_num+1} é¡µ: {str(e)}")
            except Exception as e:
                logger.error(f"PDFè§£æå¤±è´¥: {str(e)}")
                # å¦‚æœæ˜¯EOFé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨å¦ä¸€ç§æ–¹æ³•
                if "EOF" in str(e):
                    # å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–åº“å¦‚pdfmineræˆ–pdfplumber
                    logger.info("å°è¯•å¤‡ç”¨PDFè§£ææ–¹æ³•")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ å¤‡ç”¨è§£æä»£ç 
    except Exception as e:
        logger.error(f"æ— æ³•æ‰“å¼€PDFæ–‡ä»¶: {str(e)}")
    
    return text

def download_pdf_and_extract_text(paper, user_dir):
    """ä¸‹è½½PDFå¹¶æå–æ–‡æœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    pdf_path = f"{user_dir}/{paper['title']}.pdf"
    if download_pdf(paper['pdf_url'], pdf_path):
        text = extract_text_from_pdf(pdf_path, paper)
        if not text:
            logger.warning(f"è­¦å‘Š: æ— æ³•ä» {paper['title']} æå–æ–‡æœ¬")
        return text
    else:
        logger.error(f"é”™è¯¯: æ— æ³•ä¸‹è½½ {paper['title']} çš„PDF")
        return ""

def download_html_and_extract_text(paper, user_dir):
    """ä»arxivä¸‹è½½HTMLç‰ˆæœ¬ï¼Œä¿å­˜ä¸ºPDFï¼Œç„¶åæå–æ–‡æœ¬"""
    try:
        # ä»paper URLç”ŸæˆHTMLé“¾æ¥
        url = paper['url']
        if 'arxiv.org' in url:
            paper_id = url.split('/')[-1]
            html_url = f"https://arxiv.org/html/{paper_id}"
        else:
            html_url = url.replace('.pdf', '.html')

        logger.info(f"å°è¯•ä¸‹è½½HTML: {html_url}")

        # ä¸‹è½½HTMLå†…å®¹
        response = requests.get(html_url, timeout=30)

        if response.status_code == 200:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶HTMLæ–‡ä»¶
            temp_html_path = f"{user_dir}/{paper['title']}_temp.html"
            with open(temp_html_path, 'wb') as f:
                f.write(response.content)

            # ä½¿ç”¨wkhtmltopdfå°†HTMLè½¬æ¢ä¸ºPDF (éœ€è¦å®‰è£…wkhtmltopdf)
            pdf_path = f"{user_dir}/{paper['title']}_from_html.pdf"
            try:
                subprocess.run(['wkhtmltopdf', temp_html_path, pdf_path],
                              check=True, timeout=60)
                logger.info(f"å·²å°†HTMLè½¬æ¢ä¸ºPDF: {pdf_path}")
                
                # å°è¯•ä»ç”Ÿæˆçš„PDFæå–æ–‡æœ¬
                pdf_text = extract_text_from_pdf(pdf_path, paper)
                if pdf_text and len(pdf_text) > 1000:
                    return pdf_text
            except Exception as pdf_err:
                logger.error(f"HTMLè½¬PDFå¤±è´¥: {str(pdf_err)}")
            
            # å¦‚æœPDFè½¬æ¢å¤±è´¥æˆ–æå–æ–‡æœ¬ä¸è¶³ï¼Œåˆ™ç›´æ¥ä»HTMLæå–
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.extract()
                
            # è·å–æ–‡æœ¬
            text = soup.get_text(separator="\n", strip=True)
            
            # å¤„ç†ç©ºç™½å­—ç¬¦
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            logger.info(f"ä»HTMLæå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
            return text
        else:
            logger.error(f"HTMLä¸‹è½½å¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
            return ""
    except Exception as e:
        logger.error(f"HTMLå¤„ç†é”™è¯¯: {str(e)}")
        return ""

def get_paper_text(paper, user_dir):
    """å°è¯•å¤šç§æ–¹å¼è·å–è®ºæ–‡æ–‡æœ¬å†…å®¹"""
    # é¦–å…ˆå°è¯•PDFæ–¹å¼
    text = download_pdf_and_extract_text(paper, user_dir)

    # å¦‚æœPDFæ–¹å¼å¤±è´¥ï¼Œå°è¯•HTMLæ–¹å¼
    if not text or len(text) < 1000:  # å†…å®¹å¤ªå°‘å¯èƒ½æ˜¯æå–å¤±è´¥
        logger.info(f"PDFæå–å¤±è´¥æˆ–å†…å®¹å¤ªå°‘ï¼Œå°è¯•HTMLæ–¹å¼")
        text = download_html_and_extract_text(paper, user_dir)

    # å¦‚æœtexté•¿äº129024 åˆ™æˆªæ–­
    if len(text) > 129024:
        logger.warning(f"æ–‡æœ¬å†…å®¹è¿‡é•¿ï¼Œæˆªæ–­åˆ°å‰129024å­—ç¬¦")
        text = text[:129024]
    if not text:
        text = paper['abstract']  # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ‘˜è¦ä½œä¸ºæœ€åçš„fallback

    return text

def gpt_check_interest(abstract, interest_filter_prompt):
    """ä½¿ç”¨GPTåˆ¤æ–­ç”¨æˆ·æ˜¯å¦å¯¹è®ºæ–‡æ„Ÿå…´è¶£

    Args:
        abstract: è®ºæ–‡æ‘˜è¦
        interest_filter_prompt: å…´è¶£è¿‡æ»¤æç¤ºè¯ï¼Œéœ€åŒ…å«{abstract}å ä½ç¬¦

    Returns:
        tuple: (bool, dict) ç¬¬ä¸€ä¸ªå…ƒç´ è¡¨ç¤ºæ˜¯å¦æ„Ÿå…´è¶£ï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºtokenä½¿ç”¨ç»Ÿè®¡
    """
    prompt = interest_filter_prompt.format(abstract=abstract)

    client = openai.OpenAI(
        base_url=AI_CONFIG["base_url"],
        api_key=AI_CONFIG["api_key"]
    )

    logger.info(f"æ£€æŸ¥è®ºæ–‡å…´è¶£åº¦...")
    try:
        response = client.chat.completions.create(
            model=AI_CONFIG["model"],
            messages=[{
                "role": "user",
                "content": prompt
            }],
            temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„åˆ¤æ–­
        )

        # è®°å½•tokenä½¿ç”¨æƒ…å†µ
        usage = response.usage
        token_stats = {
            'prompt_tokens': usage.prompt_tokens,
            'completion_tokens': usage.completion_tokens,
            'total_tokens': usage.total_tokens
        }
        logger.info(f"Tokenä½¿ç”¨ - è¾“å…¥: {usage.prompt_tokens}, è¾“å‡º: {usage.completion_tokens}, æ€»è®¡: {usage.total_tokens}")

        answer = response.choices[0].message.content.strip().lower()
        logger.info(f"å…´è¶£åˆ¤æ–­ç»“æœ: {answer}")

        # åˆ¤æ–­AIå›å¤æ˜¯å¦è¡¨ç¤ºæ„Ÿå…´è¶£
        # æ”¯æŒå¤šç§å¯èƒ½çš„å›ç­”å½¢å¼
        interested = any(keyword in answer for keyword in ['æ˜¯', 'yes', 'æ„Ÿå…´è¶£', 'æœ‰å…´è¶£', 'interested'])
        not_interested = any(keyword in answer for keyword in ['å¦', 'no', 'ä¸æ„Ÿå…´è¶£', 'æ— å…´è¶£', 'not interested'])

        if interested and not not_interested:
            return True, token_stats
        elif not_interested and not interested:
            return False, token_stats
        else:
            # å¦‚æœæ— æ³•æ˜ç¡®åˆ¤æ–­ï¼Œé»˜è®¤ä¸ºæ„Ÿå…´è¶£ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            logger.warning(f"æ— æ³•æ˜ç¡®åˆ¤æ–­å…´è¶£ï¼Œé»˜è®¤ä¸ºæ„Ÿå…´è¶£ã€‚AIå›å¤: {answer}")
            return True, token_stats

    except Exception as e:
        logger.error(f"å…´è¶£åˆ¤æ–­å¤±è´¥: {str(e)}ï¼Œé»˜è®¤ä¸ºæ„Ÿå…´è¶£")
        return True, {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}  # å‡ºé”™æ—¶é»˜è®¤ä¸ºæ„Ÿå…´è¶£

def gpt_summarize(text, custom_prompt=None):
    """ä½¿ç”¨GPTå¯¹è®ºæ–‡è¿›è¡Œæ€»ç»“ï¼Œæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯

    Returns:
        tuple: (str, dict) ç¬¬ä¸€ä¸ªå…ƒç´ ä¸ºæ€»ç»“å†…å®¹ï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºtokenä½¿ç”¨ç»Ÿè®¡
    """
    # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
    if custom_prompt:
        prompt = custom_prompt.format(text=text)
    else:
        prompt = DEFAULT_PROMPT_TEMPLATE.format(text=text)

    client = openai.OpenAI(
        base_url=AI_CONFIG["base_url"],
        api_key=AI_CONFIG["api_key"]
    )

    logger.info(f"Requesting GPT to summarize: {text[:100]}...")
    logger.info(f"Request length: {len(text)}")
    response = client.chat.completions.create(
        model=AI_CONFIG["model"],
        messages=[{
            "role": "user",
            "content": prompt
        }],
        temperature=1.5,
    )

    # è®°å½•tokenä½¿ç”¨æƒ…å†µ
    usage = response.usage
    token_stats = {
        'prompt_tokens': usage.prompt_tokens,
        'completion_tokens': usage.completion_tokens,
        'total_tokens': usage.total_tokens
    }
    logger.info(f"Tokenä½¿ç”¨ - è¾“å…¥: {usage.prompt_tokens}, è¾“å‡º: {usage.completion_tokens}, æ€»è®¡: {usage.total_tokens}")

    logger.info(f"Response: {response.choices[0].message.content[:100]}...")
    logger.info(f"Response length: {len(response.choices[0].message.content)}")

    # Remove any code blocks from the response
    content = response.choices[0].message.content
    cleaned_content = ""
    in_code_block = False
    for line in content.split('\n'):
        if line.startswith('```'):
            in_code_block = not in_code_block
            continue
        if not in_code_block:
            cleaned_content += line + '\n'
    return response.choices[0].message.content, token_stats

def _log_token_cost(user_name, filter_input_tokens, filter_output_tokens,
                    generate_input_tokens, generate_output_tokens):
    """è®°å½•tokenä½¿ç”¨æƒ…å†µå’Œæˆæœ¬

    Args:
        user_name: ç”¨æˆ·åç§°
        filter_input_tokens: è¿‡æ»¤é˜¶æ®µè¾“å…¥tokenæ•°
        filter_output_tokens: è¿‡æ»¤é˜¶æ®µè¾“å‡ºtokenæ•°
        generate_input_tokens: ç”Ÿæˆé˜¶æ®µè¾“å…¥tokenæ•°
        generate_output_tokens: ç”Ÿæˆé˜¶æ®µè¾“å‡ºtokenæ•°
    """
    # åˆ†é˜¶æ®µç»Ÿè®¡
    filter_total = filter_input_tokens + filter_output_tokens
    generate_total = generate_input_tokens + generate_output_tokens

    # æ€»è®¡
    total_input_tokens = filter_input_tokens + generate_input_tokens
    total_output_tokens = filter_output_tokens + generate_output_tokens
    total_tokens = total_input_tokens + total_output_tokens

    # è®¡ç®—æˆæœ¬ï¼ˆå…ƒï¼‰
    filter_input_cost = (filter_input_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_input_tokens", 0)
    filter_output_cost = (filter_output_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_output_tokens", 0)
    filter_cost = filter_input_cost + filter_output_cost

    generate_input_cost = (generate_input_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_input_tokens", 0)
    generate_output_cost = (generate_output_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_output_tokens", 0)
    generate_cost = generate_input_cost + generate_output_cost

    total_cost = filter_cost + generate_cost

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("=" * 80)
    logger.info(f"ã€{user_name}ã€‘Tokenä½¿ç”¨ç»Ÿè®¡:")
    logger.info(f"")
    logger.info(f"è¿‡æ»¤é˜¶æ®µ:")
    logger.info(f"  è¾“å…¥Token: {filter_input_tokens:,}")
    logger.info(f"  è¾“å‡ºToken: {filter_output_tokens:,}")
    logger.info(f"  å°è®¡: {filter_total:,} tokens")
    logger.info(f"  æˆæœ¬: Â¥{filter_cost:.4f}")
    logger.info(f"")
    logger.info(f"ç”Ÿæˆé˜¶æ®µ:")
    logger.info(f"  è¾“å…¥Token: {generate_input_tokens:,}")
    logger.info(f"  è¾“å‡ºToken: {generate_output_tokens:,}")
    logger.info(f"  å°è®¡: {generate_total:,} tokens")
    logger.info(f"  æˆæœ¬: Â¥{generate_cost:.4f}")
    logger.info(f"")
    logger.info(f"æ€»è®¡:")
    logger.info(f"  è¾“å…¥Token: {total_input_tokens:,}")
    logger.info(f"  è¾“å‡ºToken: {total_output_tokens:,}")
    logger.info(f"  æ€»Tokenæ•°: {total_tokens:,}")
    logger.info(f"  æ€»æˆæœ¬: Â¥{total_cost:.4f}")
    logger.info("=" * 80)

def build_filtered_papers_appendix(filtered_out_papers):
    """æ„å»ºè¢«è¿‡æ»¤è®ºæ–‡çš„é™„å½•

    Args:
        filtered_out_papers: è¢«è¿‡æ»¤æ‰çš„è®ºæ–‡åˆ—è¡¨

    Returns:
        str: æ ¼å¼åŒ–çš„é™„å½•å†…å®¹
    """
    if not filtered_out_papers:
        return ""

    appendix = ["\n\n" + "=" * 80]
    appendix.append("\n## ğŸ“‹ é™„å½•ï¼šå…¶ä»–è®ºæ–‡ï¼ˆæœªé€šè¿‡å…´è¶£è¿‡æ»¤ï¼‰")
    appendix.append("\nä»¥ä¸‹è®ºæ–‡æœªé€šè¿‡AIå…´è¶£è¿‡æ»¤ï¼Œä»…ä¾›å‚è€ƒå®¡æŸ¥ï¼š\n")

    for i, paper in enumerate(filtered_out_papers, 1):
        appendix.append(f"\n### {i}. {paper['title']}\n")
        appendix.append(f"**ä½œè€…**: {', '.join(paper['authors'])}\n")
        appendix.append(f"**å‘è¡¨æ—¥æœŸ**: {paper['published'].strftime('%Y-%m-%d')}\n")
        appendix.append(f"**é“¾æ¥**: [{paper['url']}]({paper['url']})\n")
        appendix.append(f"**ä¸»è¦åˆ†ç±»**: {paper.get('primary_category', 'æœªçŸ¥åˆ†ç±»')}\n")
        appendix.append(f"\n**æ‘˜è¦**:\n{paper['abstract']}\n")
        appendix.append("\n" + "â”€" * 80 + "\n")

    return ''.join(appendix)

def process_user(user_config):
    """å¤„ç†å•ä¸ªç”¨æˆ·çš„è®ºæ–‡è·å–å’ŒæŠ¥å‘Šç”Ÿæˆ"""
    user_name = user_config["name"]
    user_email = user_config["email"]
    arxiv_categories = user_config["arxiv_categories"]
    custom_prompt = user_config.get("custom_prompt", None)
    interest_filter_prompt = user_config.get("interest_filter_prompt", None)

    logger.info(f"å¼€å§‹å¤„ç†ç”¨æˆ·: {user_name}")

    # åˆå§‹åŒ–tokenç»Ÿè®¡ - åˆ†é˜¶æ®µç»Ÿè®¡
    filter_input_tokens = 0
    filter_output_tokens = 0
    generate_input_tokens = 0
    generate_output_tokens = 0

    # åˆå§‹åŒ–è®ºæ–‡æ•°é‡ç»Ÿè®¡
    papers_fetched = 0
    papers_filtered_count = 0
    papers_processed_count = 0

    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶ç›®å½•
    user_dir = f"temp/{user_name.replace(' ', '_')}"
    os.makedirs(user_dir, exist_ok=True)

    # è·å–è¯¥ç”¨æˆ·å…³æ³¨çš„è®ºæ–‡
    papers = fetch_papers(arxiv_categories)
    papers_fetched = len(papers)

    if not papers:
        logger.info(f"ç”¨æˆ· {user_name} æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")
        return

    # ç¬¬ä¸€æ­¥ï¼šå¦‚æœé…ç½®äº†å…´è¶£è¿‡æ»¤æç¤ºè¯ï¼Œå…ˆæ ¹æ®æ‘˜è¦è¿‡æ»¤è®ºæ–‡
    filtered_out_papers = []  # å­˜å‚¨è¢«è¿‡æ»¤æ‰çš„è®ºæ–‡
    if interest_filter_prompt:
        logger.info(f"å¼€å§‹ä½¿ç”¨å…´è¶£è¿‡æ»¤ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰ï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡å¾…è¿‡æ»¤")
        filtered_papers = []

        # å®šä¹‰å•ä¸ªè®ºæ–‡è¿‡æ»¤ä»»åŠ¡
        def filter_single_paper(paper_with_index):
            i, paper = paper_with_index
            logger.info(f"è¿‡æ»¤è®ºæ–‡ {i+1}/{len(papers)}: {paper['title']}")
            try:
                is_interested, token_stats = gpt_check_interest(paper['abstract'], interest_filter_prompt)
                if is_interested:
                    logger.info(f"âœ“ ç”¨æˆ·å¯èƒ½å¯¹æ­¤è®ºæ–‡æ„Ÿå…´è¶£")
                    return ('interested', paper, token_stats)
                else:
                    logger.info(f"âœ— ç”¨æˆ·å¯èƒ½å¯¹æ­¤è®ºæ–‡ä¸æ„Ÿå…´è¶£ï¼Œè·³è¿‡")
                    return ('not_interested', paper, token_stats)
            except Exception as e:
                logger.error(f"è¿‡æ»¤è®ºæ–‡æ—¶å‡ºé”™: {str(e)}ï¼Œä¿ç•™è¯¥è®ºæ–‡")
                return ('error', paper, None)

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘è¿‡æ»¤ï¼ˆé™ä½å¹¶å‘æ•°é¿å…APIé™æµï¼‰
        with ThreadPoolExecutor(max_workers=3) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_paper = {executor.submit(filter_single_paper, (i, paper)): paper
                              for i, paper in enumerate(papers)}

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_paper):
                try:
                    result_type, paper, token_stats = future.result()

                    # ç´¯è®¡tokenä½¿ç”¨
                    if token_stats:
                        filter_input_tokens += token_stats['prompt_tokens']
                        filter_output_tokens += token_stats['completion_tokens']

                    if result_type == 'interested' or result_type == 'error':
                        filtered_papers.append(paper)
                    else:  # not_interested
                        filtered_out_papers.append(paper)

                except Exception as e:
                    logger.error(f"å¤„ç†è¿‡æ»¤ç»“æœæ—¶å‡ºé”™: {str(e)}")

        papers = filtered_papers
        papers_filtered_count = len(papers)
        logger.info(f"å…´è¶£è¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {len(papers)} ç¯‡è®ºæ–‡ï¼Œè¿‡æ»¤æ‰ {len(filtered_out_papers)} ç¯‡è®ºæ–‡")

        if not papers:
            logger.info(f"ç”¨æˆ· {user_name} ç»è¿‡å…´è¶£è¿‡æ»¤åæ²¡æœ‰æ„Ÿå…´è¶£çš„è®ºæ–‡")
            # è®¡ç®—æˆæœ¬
            filter_input_cost = (filter_input_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_input_tokens", 0)
            filter_output_cost = (filter_output_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_output_tokens", 0)
            filter_cost = filter_input_cost + filter_output_cost

            # è®°å½•åˆ°æ•°æ®åº“
            try:
                db = get_db()
                db.record_usage(
                    user_name=user_name,
                    user_email=user_email,
                    arxiv_categories=arxiv_categories,
                    filter_input_tokens=filter_input_tokens,
                    filter_output_tokens=filter_output_tokens,
                    generate_input_tokens=0,
                    generate_output_tokens=0,
                    filter_cost=filter_cost,
                    generate_cost=0.0,
                    papers_fetched=papers_fetched,
                    papers_filtered=0,
                    papers_processed=0
                )
            except Exception as e:
                logger.error(f"è®°å½•æ•°æ®åº“å¤±è´¥: {str(e)}")

            # è¾“å‡ºæˆæœ¬ç»Ÿè®¡
            _log_token_cost(user_name, filter_input_tokens, filter_output_tokens,
                           generate_input_tokens, generate_output_tokens)
            # å³ä½¿æ²¡æœ‰æ„Ÿå…´è¶£çš„è®ºæ–‡ï¼Œå¦‚æœæœ‰è¢«è¿‡æ»¤çš„è®ºæ–‡ï¼Œä¹Ÿå‘é€é™„å½•
            if filtered_out_papers:
                filtered_appendix = build_filtered_papers_appendix(filtered_out_papers)
                asyncio.run(send_email(f"æ¯æ—¥ArXivè®ºæ–‡æŠ¥å‘Š - {user_name}", filtered_appendix, user_email))
            return
    else:
        # æ²¡æœ‰é…ç½®å…´è¶£è¿‡æ»¤ï¼Œæ‰€æœ‰è®ºæ–‡éƒ½é€šè¿‡
        papers_filtered_count = len(papers)

    # ç¬¬äºŒæ­¥ï¼šæ ¹æ®é…ç½®é™åˆ¶å¤„ç†çš„è®ºæ–‡æ•°é‡ï¼ˆç¡¬æˆªæ–­ï¼‰
    max_papers = GENERAL_CONFIG.get("max_papers_per_user", None)
    if max_papers is not None and max_papers > 0:
        papers = papers[:max_papers]
        logger.info(f"åº”ç”¨ç¡¬æˆªæ–­ï¼Œç”¨æˆ· {user_name} æœ€å¤šå¤„ç† {max_papers} ç¯‡è®ºæ–‡")

    report = []
    papers_processed_count = 0
    for paper in papers:
        try:
            # ä¸‹è½½å¹¶å¤„ç†PDF
            text = get_paper_text(paper, user_dir)

            # GPTæ€»ç»“ï¼ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ï¼‰
            summary, token_stats = gpt_summarize(text, custom_prompt)
            # ç´¯è®¡ç”Ÿæˆé˜¶æ®µtokenä½¿ç”¨
            generate_input_tokens += token_stats['prompt_tokens']
            generate_output_tokens += token_stats['completion_tokens']
            papers_processed_count += 1

            # æ„å»ºæŠ¥å‘Š
            report.append(f"""
## ğŸ“„è®ºæ–‡æ ‡é¢˜

{paper['title']}

## ğŸ“Š è®ºæ–‡ä¿¡æ¯
* **ä½œè€…**: {', '.join(paper['authors'])}
* **å‘è¡¨æ—¥æœŸ**: {paper['published'].strftime('%Y-%m-%d')}
* **é“¾æ¥**: [{paper['url']}]({paper['url']})
* **ä¸»è¦åˆ†ç±»**: {paper["primary_category"] if "primary_category" in paper else "æœªçŸ¥åˆ†ç±»"}
* **æ‰€å±åˆ†ç±»**: {paper["categories"] if "categories" in paper else "æœªçŸ¥åˆ†ç±»"}
* **æ‘˜è¦åŸæ–‡**:

{paper['abstract']}


## ğŸ“ è®ºæ–‡æ€»ç»“
{summary}

{'â”€' * 80}
""")
        except Exception as e:
            logger.error(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")
            report.append(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")

    # è¾“å‡ºç”¨æˆ·çš„tokenä½¿ç”¨ç»Ÿè®¡å’Œæˆæœ¬
    _log_token_cost(user_name, filter_input_tokens, filter_output_tokens,
                   generate_input_tokens, generate_output_tokens)

    # è®¡ç®—æˆæœ¬
    filter_input_cost = (filter_input_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_input_tokens", 0)
    filter_output_cost = (filter_output_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_output_tokens", 0)
    filter_cost = filter_input_cost + filter_output_cost

    generate_input_cost = (generate_input_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_input_tokens", 0)
    generate_output_cost = (generate_output_tokens / 1_000_000) * AI_CONFIG.get("price_per_million_output_tokens", 0)
    generate_cost = generate_input_cost + generate_output_cost

    # è®°å½•åˆ°æ•°æ®åº“
    try:
        db = get_db()
        db.record_usage(
            user_name=user_name,
            user_email=user_email,
            arxiv_categories=arxiv_categories,
            filter_input_tokens=filter_input_tokens,
            filter_output_tokens=filter_output_tokens,
            generate_input_tokens=generate_input_tokens,
            generate_output_tokens=generate_output_tokens,
            filter_cost=filter_cost,
            generate_cost=generate_cost,
            papers_fetched=papers_fetched,
            papers_filtered=papers_filtered_count,
            papers_processed=papers_processed_count
        )
    except Exception as e:
        logger.error(f"è®°å½•æ•°æ®åº“å¤±è´¥: {str(e)}")

    if report:
        # æ„å»ºå®Œæ•´æŠ¥å‘Šï¼ŒåŒ…æ‹¬è¢«è¿‡æ»¤è®ºæ–‡çš„é™„å½•
        full_report = '\n'.join(report)

        # å¦‚æœæœ‰è¢«è¿‡æ»¤æ‰çš„è®ºæ–‡ï¼Œæ·»åŠ é™„å½•
        if filtered_out_papers:
            full_report += "\n\n" + build_filtered_papers_appendix(filtered_out_papers)

        # å‘é€ç»™è¯¥ç”¨æˆ·
        asyncio.run(send_email(f"æ¯æ—¥ArXivè®ºæ–‡æŠ¥å‘Š - {user_name}", full_report, user_email))

        # ä¿å­˜æŠ¥å‘Šåˆ°ç”¨æˆ·ä¸“å±æ–‡ä»¶
        report_file = f"{user_dir}/report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(full_report)
        logger.success(f"ç”¨æˆ· {user_name} çš„æŠ¥å‘Šå·²å‘é€å¹¶ä¿å­˜åˆ° {report_file}")

def daily_job():
    """æ¯æ—¥ä»»åŠ¡ï¼šä¸ºæ‰€æœ‰é…ç½®çš„ç”¨æˆ·å¤„ç†è®ºæ–‡"""
    os.makedirs('temp', exist_ok=True)

    logger.info(f"å¼€å§‹æ¯æ—¥ä»»åŠ¡ï¼Œå…±æœ‰ {len(USERS_CONFIG)} ä¸ªç”¨æˆ·")

    for i, user_config in enumerate(USERS_CONFIG):
        try:
            process_user(user_config)
            # åœ¨å¤„ç†ç”¨æˆ·ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…ArXiv APIé™æµ
            if i < len(USERS_CONFIG) - 1:
                logger.info(f"ç­‰å¾…60ç§’åå¤„ç†ä¸‹ä¸€ä¸ªç”¨æˆ·ï¼Œé¿å…APIé™æµ...")
                time.sleep(60)
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ· {user_config['name']} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    logger.success("æ‰€æœ‰ç”¨æˆ·å¤„ç†å®Œæˆ")

def run_scheduler():
    scheduler = BlockingScheduler()
    scheduler.add_job(
        daily_job, 
        trigger=CronTrigger(hour=16, minute=0),  # æ¯å¤©ä¸‹åˆ4ç‚¹è¿è¡Œ
        id='daily_arxiv_job',
        name='Daily ArXiv paper collection and summary'
    )
    
    logger.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯å¤©ä¸‹åˆ4:00è¿è¡Œ")
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")

if __name__ == "__main__":
    # é…ç½®loguru
    logger.add(
        "arxiv_pusher.log",
        rotation="10 MB",
        level="INFO",
        encoding="utf-8"
    )
    # å¦‚æœéœ€è¦ç«‹å³è¿è¡Œä¸€æ¬¡ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # daily_job()
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    run_scheduler()