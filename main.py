import os
import requests
from datetime import datetime, timedelta
from arxiv import Client, Search, SortCriterion, SortOrder
from PyPDF2 import PdfReader
import openai

from config import AI_CONFIG, EMAIL_SERVER_CONFIG, GENERAL_CONFIG, USERS_CONFIG, DEFAULT_PROMPT_TEMPLATE

import smtplib
import socket
import asyncio
from email.mime.text import MIMEText
import markdown2  # å¯¼å…¥markdown2åº“
from loguru import logger
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
import time
from bs4 import BeautifulSoup
import subprocess



async def send_email(subject, content, receiver_email):
    """å‘é€é‚®ä»¶é€šçŸ¥ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
    # å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML
    html_content = markdown2.markdown(content, extras=["tables", "mathjax", "fenced-code-blocks"])
    msg = MIMEText(html_content, "html", "utf-8")
    msg["Subject"] = subject
    msg["From"] = EMAIL_SERVER_CONFIG["sender"]
    msg["To"] = receiver_email

    server = None
    try:
        logger.info(f"æ­£åœ¨è¿æ¥SMTPæœåŠ¡å™¨ï¼Œå‘é€ç»™ {receiver_email}...")
        # å°†SMTPæ“ä½œæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        return await asyncio.get_event_loop().run_in_executor(
            None, lambda: _send_email_sync(msg, server, receiver_email)
        )
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False


def _send_email_sync(msg, server=None, receiver_email=None):
    """åŒæ­¥å‘é€é‚®ä»¶çš„å†…éƒ¨å‡½æ•°"""
    try:
        server = smtplib.SMTP(
            EMAIL_SERVER_CONFIG["smtp_server"], EMAIL_SERVER_CONFIG["smtp_port"], timeout=10
        )
        server.starttls()  # å¯ç”¨TLSåŠ å¯†
        server.login(EMAIL_SERVER_CONFIG["sender"], EMAIL_SERVER_CONFIG["password"])

        if receiver_email.count(",") > 0:
            receivers = receiver_email.split(",")
            server.sendmail(EMAIL_SERVER_CONFIG["sender"], receivers, msg.as_string())
        else:
            server.sendmail(
                EMAIL_SERVER_CONFIG["sender"], [receiver_email], msg.as_string()
            )

        logger.success("é‚®ä»¶å‘é€æˆåŠŸ")
        return True
    except socket.timeout:
        logger.warning("è¿æ¥SMTPæœåŠ¡å™¨è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡é‚®ä»¶å‘é€")
        return False
    except smtplib.SMTPException as e:
        logger.error(
            f"SMTPé”™è¯¯: {e.smtp_error.decode() if hasattr(e, 'smtp_error') else str(e)}"
        )
        return False
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False
    finally:
        if server:
            try:
                server.quit()
            except Exception as e:
                logger.warning(f"å…³é—­SMTPè¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


def fetch_papers(arxiv_categories):
    """è·å–æŒ‡å®šåˆ†ç±»çš„è®ºæ–‡"""
    # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ŒåªåŒ…å«é…ç½®ä¸­çš„ä¸»é¢˜
    search_query = " OR ".join([f"cat:{cat}" for cat in arxiv_categories])
    client = Client()  # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    search = Search(
        query=search_query,
        sort_by=SortCriterion.SubmittedDate,
        sort_order=SortOrder.Descending,
        max_results=100
    )

    papers = []
    # Get the target date (previous workday)
    today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    target_date = today - timedelta(days=GENERAL_CONFIG["days_lookback"])

    # Adjust if yesterday was a weekend
    weekday = target_date.weekday()  # 0-6, where 5 is Saturday and 6 is Sunday
    if weekday >= 5:  # If Saturday or Sunday
        # Go back to Friday (4)
        target_date -= timedelta(days=weekday - 4)

    logger.info(f"Target date set to previous workday: {target_date.strftime('%Y-%m-%d')}")
    for result in client.results(search):
        logger.info(f"Processing paper: {result.title} published on {result.published}")
        # Check if the paper was published on the target date
        published_dt = result.published.replace(tzinfo=None)
        if target_date <= published_dt :
            papers.append({
                "title": result.title,
                "url": result.entry_id,
                "pdf_url": result.pdf_url,
                "abstract": result.summary,
                "authors": [a.name for a in result.authors],
                "published": result.published,
                "categories": [c for c in result.categories],
                "primary_category": result.primary_category if result.primary_category else None
            })
    logger.success(f"Found {len(papers)} papers published from {target_date.strftime('%Y-%m-%d')}")
    return papers

def download_pdf(url, filename, max_retries=3):
    """ä¸‹è½½PDFæ–‡ä»¶ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""
        # ç¡®ä¿URLæ˜¯æ­£ç¡®çš„PDFé“¾æ¥
    if 'arxiv.org' in url and not url.endswith('.pdf'):
        # ä»URLæå–è®ºæ–‡ID
        paper_id = url.split('/')[-1]
        url = f"https://arxiv.org/pdf/{paper_id}.pdf"
    
    logger.info(f"å°è¯•ä¸‹è½½: {url}")
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)  # æ·»åŠ è¶…æ—¶å‚æ•°
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸä¸”å†…å®¹ç±»å‹æ˜¯PDF
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '')
                if 'pdf' not in content_type.lower() and len(response.content) < 10000:
                    logger.warning(f"å“åº”å¯èƒ½ä¸æ˜¯PDFæ–‡ä»¶ (Content-Type: {content_type})")
                
                with open(filename, 'wb') as f:
                    f.write(response.content)
                
                # éªŒè¯æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(filename)
                if file_size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
                    logger.warning(f"ä¸‹è½½çš„æ–‡ä»¶è¿‡å° ({file_size} å­—èŠ‚)")
                    continue
                
                return True
            else:
                logger.error(f"ä¸‹è½½å¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            logger.warning(f"å°è¯• {attempt+1}/{max_retries} å¤±è´¥: {str(e)}")
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
        if attempt < max_retries - 1:
            time.sleep(2 * (attempt + 1))  # æŒ‡æ•°é€€é¿
    
    return False

def extract_text_from_pdf(pdf_path, paper):
    """ä»PDFæå–æ–‡æœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    text = ""
    try:
        with open(pdf_path, 'rb') as f:
            try:
                reader = PdfReader(f)
                for page_num, page in enumerate(reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except Exception as e:
                        logger.warning(f"æ— æ³•æå–ç¬¬ {page_num+1} é¡µ: {str(e)}")
            except Exception as e:
                logger.error(f"PDFè§£æå¤±è´¥: {str(e)}")
                # å¦‚æœæ˜¯EOFé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨å¦ä¸€ç§æ–¹æ³•
                if "EOF" in str(e):
                    # å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–åº“å¦‚pdfmineræˆ–pdfplumber
                    logger.info("å°è¯•å¤‡ç”¨PDFè§£ææ–¹æ³•")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ å¤‡ç”¨è§£æä»£ç 
    except Exception as e:
        logger.error(f"æ— æ³•æ‰“å¼€PDFæ–‡ä»¶: {str(e)}")
    
    return text

def download_pdf_and_extract_text(paper, user_dir):
    """ä¸‹è½½PDFå¹¶æå–æ–‡æœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    pdf_path = f"{user_dir}/{paper['title']}.pdf"
    if download_pdf(paper['pdf_url'], pdf_path):
        text = extract_text_from_pdf(pdf_path, paper)
        if not text:
            logger.warning(f"è­¦å‘Š: æ— æ³•ä» {paper['title']} æå–æ–‡æœ¬")
        return text
    else:
        logger.error(f"é”™è¯¯: æ— æ³•ä¸‹è½½ {paper['title']} çš„PDF")
        return ""

def download_html_and_extract_text(paper, user_dir):
    """ä»arxivä¸‹è½½HTMLç‰ˆæœ¬ï¼Œä¿å­˜ä¸ºPDFï¼Œç„¶åæå–æ–‡æœ¬"""
    try:
        # ä»paper URLç”ŸæˆHTMLé“¾æ¥
        url = paper['url']
        if 'arxiv.org' in url:
            paper_id = url.split('/')[-1]
            html_url = f"https://arxiv.org/html/{paper_id}"
        else:
            html_url = url.replace('.pdf', '.html')

        logger.info(f"å°è¯•ä¸‹è½½HTML: {html_url}")

        # ä¸‹è½½HTMLå†…å®¹
        response = requests.get(html_url, timeout=30)

        if response.status_code == 200:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶HTMLæ–‡ä»¶
            temp_html_path = f"{user_dir}/{paper['title']}_temp.html"
            with open(temp_html_path, 'wb') as f:
                f.write(response.content)

            # ä½¿ç”¨wkhtmltopdfå°†HTMLè½¬æ¢ä¸ºPDF (éœ€è¦å®‰è£…wkhtmltopdf)
            pdf_path = f"{user_dir}/{paper['title']}_from_html.pdf"
            try:
                subprocess.run(['wkhtmltopdf', temp_html_path, pdf_path],
                              check=True, timeout=60)
                logger.info(f"å·²å°†HTMLè½¬æ¢ä¸ºPDF: {pdf_path}")
                
                # å°è¯•ä»ç”Ÿæˆçš„PDFæå–æ–‡æœ¬
                pdf_text = extract_text_from_pdf(pdf_path, paper)
                if pdf_text and len(pdf_text) > 1000:
                    return pdf_text
            except Exception as pdf_err:
                logger.error(f"HTMLè½¬PDFå¤±è´¥: {str(pdf_err)}")
            
            # å¦‚æœPDFè½¬æ¢å¤±è´¥æˆ–æå–æ–‡æœ¬ä¸è¶³ï¼Œåˆ™ç›´æ¥ä»HTMLæå–
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.extract()
                
            # è·å–æ–‡æœ¬
            text = soup.get_text(separator="\n", strip=True)
            
            # å¤„ç†ç©ºç™½å­—ç¬¦
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            logger.info(f"ä»HTMLæå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
            return text
        else:
            logger.error(f"HTMLä¸‹è½½å¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
            return ""
    except Exception as e:
        logger.error(f"HTMLå¤„ç†é”™è¯¯: {str(e)}")
        return ""

def get_paper_text(paper, user_dir):
    """å°è¯•å¤šç§æ–¹å¼è·å–è®ºæ–‡æ–‡æœ¬å†…å®¹"""
    # é¦–å…ˆå°è¯•PDFæ–¹å¼
    text = download_pdf_and_extract_text(paper, user_dir)

    # å¦‚æœPDFæ–¹å¼å¤±è´¥ï¼Œå°è¯•HTMLæ–¹å¼
    if not text or len(text) < 1000:  # å†…å®¹å¤ªå°‘å¯èƒ½æ˜¯æå–å¤±è´¥
        logger.info(f"PDFæå–å¤±è´¥æˆ–å†…å®¹å¤ªå°‘ï¼Œå°è¯•HTMLæ–¹å¼")
        text = download_html_and_extract_text(paper, user_dir)

    # å¦‚æœtexté•¿äº129024 åˆ™æˆªæ–­
    if len(text) > 129024:
        logger.warning(f"æ–‡æœ¬å†…å®¹è¿‡é•¿ï¼Œæˆªæ–­åˆ°å‰129024å­—ç¬¦")
        text = text[:129024]
    if not text:
        text = paper['abstract']  # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ‘˜è¦ä½œä¸ºæœ€åçš„fallback

    return text

def gpt_check_interest(abstract, interest_filter_prompt):
    """ä½¿ç”¨GPTåˆ¤æ–­ç”¨æˆ·æ˜¯å¦å¯¹è®ºæ–‡æ„Ÿå…´è¶£

    Args:
        abstract: è®ºæ–‡æ‘˜è¦
        interest_filter_prompt: å…´è¶£è¿‡æ»¤æç¤ºè¯ï¼Œéœ€åŒ…å«{abstract}å ä½ç¬¦

    Returns:
        bool: Trueè¡¨ç¤ºæ„Ÿå…´è¶£ï¼ŒFalseè¡¨ç¤ºä¸æ„Ÿå…´è¶£
    """
    prompt = interest_filter_prompt.format(abstract=abstract)

    client = openai.OpenAI(
        base_url=AI_CONFIG["base_url"],
        api_key=AI_CONFIG["api_key"]
    )

    logger.info(f"æ£€æŸ¥è®ºæ–‡å…´è¶£åº¦...")
    try:
        response = client.chat.completions.create(
            model=AI_CONFIG["model"],
            messages=[{
                "role": "user",
                "content": prompt
            }],
            temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„åˆ¤æ–­
        )

        answer = response.choices[0].message.content.strip().lower()
        logger.info(f"å…´è¶£åˆ¤æ–­ç»“æœ: {answer}")

        # åˆ¤æ–­AIå›å¤æ˜¯å¦è¡¨ç¤ºæ„Ÿå…´è¶£
        # æ”¯æŒå¤šç§å¯èƒ½çš„å›ç­”å½¢å¼
        interested = any(keyword in answer for keyword in ['æ˜¯', 'yes', 'æ„Ÿå…´è¶£', 'æœ‰å…´è¶£', 'interested'])
        not_interested = any(keyword in answer for keyword in ['å¦', 'no', 'ä¸æ„Ÿå…´è¶£', 'æ— å…´è¶£', 'not interested'])

        if interested and not not_interested:
            return True
        elif not_interested and not interested:
            return False
        else:
            # å¦‚æœæ— æ³•æ˜ç¡®åˆ¤æ–­ï¼Œé»˜è®¤ä¸ºæ„Ÿå…´è¶£ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            logger.warning(f"æ— æ³•æ˜ç¡®åˆ¤æ–­å…´è¶£ï¼Œé»˜è®¤ä¸ºæ„Ÿå…´è¶£ã€‚AIå›å¤: {answer}")
            return True

    except Exception as e:
        logger.error(f"å…´è¶£åˆ¤æ–­å¤±è´¥: {str(e)}ï¼Œé»˜è®¤ä¸ºæ„Ÿå…´è¶£")
        return True  # å‡ºé”™æ—¶é»˜è®¤ä¸ºæ„Ÿå…´è¶£

def gpt_summarize(text, custom_prompt=None):
    """ä½¿ç”¨GPTå¯¹è®ºæ–‡è¿›è¡Œæ€»ç»“ï¼Œæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯"""
    # å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰æç¤ºè¯ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
    if custom_prompt:
        prompt = custom_prompt.format(text=text)
    else:
        prompt = DEFAULT_PROMPT_TEMPLATE.format(text=text)

    client = openai.OpenAI(
        base_url=AI_CONFIG["base_url"],
        api_key=AI_CONFIG["api_key"]
    )

    logger.info(f"Requesting GPT to summarize: {text[:100]}...")
    logger.info(f"Request length: {len(text)}")
    response = client.chat.completions.create(
        model=AI_CONFIG["model"],
        messages=[{
            "role": "user",
            "content": prompt
        }],
        temperature=1.5,
    )
    logger.info(f"Response: {response.choices[0].message.content[:100]}...")
    logger.info(f"Response length: {len(response.choices[0].message.content)}")

    # Remove any code blocks from the response
    content = response.choices[0].message.content
    cleaned_content = ""
    in_code_block = False
    for line in content.split('\n'):
        if line.startswith('```'):
            in_code_block = not in_code_block
            continue
        if not in_code_block:
            cleaned_content += line + '\n'
    return response.choices[0].message.content

def build_filtered_papers_appendix(filtered_out_papers):
    """æ„å»ºè¢«è¿‡æ»¤è®ºæ–‡çš„é™„å½•

    Args:
        filtered_out_papers: è¢«è¿‡æ»¤æ‰çš„è®ºæ–‡åˆ—è¡¨

    Returns:
        str: æ ¼å¼åŒ–çš„é™„å½•å†…å®¹
    """
    if not filtered_out_papers:
        return ""

    appendix = ["\n\n" + "=" * 80]
    appendix.append("\n## ğŸ“‹ é™„å½•ï¼šå…¶ä»–è®ºæ–‡ï¼ˆæœªé€šè¿‡å…´è¶£è¿‡æ»¤ï¼‰")
    appendix.append("\nä»¥ä¸‹è®ºæ–‡æœªé€šè¿‡AIå…´è¶£è¿‡æ»¤ï¼Œä»…ä¾›å‚è€ƒå®¡æŸ¥ï¼š\n")

    for i, paper in enumerate(filtered_out_papers, 1):
        appendix.append(f"\n### {i}. {paper['title']}\n")
        appendix.append(f"**ä½œè€…**: {', '.join(paper['authors'])}\n")
        appendix.append(f"**å‘è¡¨æ—¥æœŸ**: {paper['published'].strftime('%Y-%m-%d')}\n")
        appendix.append(f"**é“¾æ¥**: [{paper['url']}]({paper['url']})\n")
        appendix.append(f"**ä¸»è¦åˆ†ç±»**: {paper.get('primary_category', 'æœªçŸ¥åˆ†ç±»')}\n")
        appendix.append(f"\n**æ‘˜è¦**:\n{paper['abstract']}\n")
        appendix.append("\n" + "â”€" * 80 + "\n")

    return ''.join(appendix)

def process_user(user_config):
    """å¤„ç†å•ä¸ªç”¨æˆ·çš„è®ºæ–‡è·å–å’ŒæŠ¥å‘Šç”Ÿæˆ"""
    user_name = user_config["name"]
    user_email = user_config["email"]
    arxiv_categories = user_config["arxiv_categories"]
    custom_prompt = user_config.get("custom_prompt", None)
    interest_filter_prompt = user_config.get("interest_filter_prompt", None)

    logger.info(f"å¼€å§‹å¤„ç†ç”¨æˆ·: {user_name}")

    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶ç›®å½•
    user_dir = f"temp/{user_name.replace(' ', '_')}"
    os.makedirs(user_dir, exist_ok=True)

    # è·å–è¯¥ç”¨æˆ·å…³æ³¨çš„è®ºæ–‡
    papers = fetch_papers(arxiv_categories)

    if not papers:
        logger.info(f"ç”¨æˆ· {user_name} æ²¡æœ‰æ‰¾åˆ°æ–°è®ºæ–‡")
        return

    # ç¬¬ä¸€æ­¥ï¼šå¦‚æœé…ç½®äº†å…´è¶£è¿‡æ»¤æç¤ºè¯ï¼Œå…ˆæ ¹æ®æ‘˜è¦è¿‡æ»¤è®ºæ–‡
    filtered_out_papers = []  # å­˜å‚¨è¢«è¿‡æ»¤æ‰çš„è®ºæ–‡
    if interest_filter_prompt:
        logger.info(f"å¼€å§‹ä½¿ç”¨å…´è¶£è¿‡æ»¤ï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡å¾…è¿‡æ»¤")
        filtered_papers = []
        for i, paper in enumerate(papers):
            logger.info(f"è¿‡æ»¤è®ºæ–‡ {i+1}/{len(papers)}: {paper['title']}")
            try:
                is_interested = gpt_check_interest(paper['abstract'], interest_filter_prompt)
                if is_interested:
                    filtered_papers.append(paper)
                    logger.info(f"âœ“ ç”¨æˆ·å¯èƒ½å¯¹æ­¤è®ºæ–‡æ„Ÿå…´è¶£")
                else:
                    filtered_out_papers.append(paper)
                    logger.info(f"âœ— ç”¨æˆ·å¯èƒ½å¯¹æ­¤è®ºæ–‡ä¸æ„Ÿå…´è¶£ï¼Œè·³è¿‡")
            except Exception as e:
                logger.error(f"è¿‡æ»¤è®ºæ–‡æ—¶å‡ºé”™: {str(e)}ï¼Œä¿ç•™è¯¥è®ºæ–‡")
                filtered_papers.append(paper)

        papers = filtered_papers
        logger.info(f"å…´è¶£è¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {len(papers)} ç¯‡è®ºæ–‡ï¼Œè¿‡æ»¤æ‰ {len(filtered_out_papers)} ç¯‡è®ºæ–‡")

        if not papers:
            logger.info(f"ç”¨æˆ· {user_name} ç»è¿‡å…´è¶£è¿‡æ»¤åæ²¡æœ‰æ„Ÿå…´è¶£çš„è®ºæ–‡")
            # å³ä½¿æ²¡æœ‰æ„Ÿå…´è¶£çš„è®ºæ–‡ï¼Œå¦‚æœæœ‰è¢«è¿‡æ»¤çš„è®ºæ–‡ï¼Œä¹Ÿå‘é€é™„å½•
            if filtered_out_papers:
                filtered_appendix = build_filtered_papers_appendix(filtered_out_papers)
                asyncio.run(send_email(f"æ¯æ—¥ArXivè®ºæ–‡æŠ¥å‘Š - {user_name}", filtered_appendix, user_email))
            return

    # ç¬¬äºŒæ­¥ï¼šæ ¹æ®é…ç½®é™åˆ¶å¤„ç†çš„è®ºæ–‡æ•°é‡ï¼ˆç¡¬æˆªæ–­ï¼‰
    max_papers = GENERAL_CONFIG.get("max_papers_per_user", None)
    if max_papers is not None and max_papers > 0:
        papers = papers[:max_papers]
        logger.info(f"åº”ç”¨ç¡¬æˆªæ–­ï¼Œç”¨æˆ· {user_name} æœ€å¤šå¤„ç† {max_papers} ç¯‡è®ºæ–‡")

    report = []
    for paper in papers:
        try:
            # ä¸‹è½½å¹¶å¤„ç†PDF
            text = get_paper_text(paper, user_dir)

            # GPTæ€»ç»“ï¼ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ï¼‰
            summary = gpt_summarize(text, custom_prompt)

            # æ„å»ºæŠ¥å‘Š
            report.append(f"""
## ğŸ“„è®ºæ–‡æ ‡é¢˜

{paper['title']}

## ğŸ“Š è®ºæ–‡ä¿¡æ¯
* **ä½œè€…**: {', '.join(paper['authors'])}
* **å‘è¡¨æ—¥æœŸ**: {paper['published'].strftime('%Y-%m-%d')}
* **é“¾æ¥**: [{paper['url']}]({paper['url']})
* **ä¸»è¦åˆ†ç±»**: {paper["primary_category"] if "primary_category" in paper else "æœªçŸ¥åˆ†ç±»"}
* **æ‰€å±åˆ†ç±»**: {paper["categories"] if "categories" in paper else "æœªçŸ¥åˆ†ç±»"}
* **æ‘˜è¦åŸæ–‡**: 

{paper['abstract']}


## ğŸ“ è®ºæ–‡æ€»ç»“
{summary}

{'â”€' * 80}
""")
        except Exception as e:
            logger.error(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")
            report.append(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")

    if report:
        # æ„å»ºå®Œæ•´æŠ¥å‘Šï¼ŒåŒ…æ‹¬è¢«è¿‡æ»¤è®ºæ–‡çš„é™„å½•
        full_report = '\n'.join(report)

        # å¦‚æœæœ‰è¢«è¿‡æ»¤æ‰çš„è®ºæ–‡ï¼Œæ·»åŠ é™„å½•
        if filtered_out_papers:
            full_report += "\n\n" + build_filtered_papers_appendix(filtered_out_papers)

        # å‘é€ç»™è¯¥ç”¨æˆ·
        asyncio.run(send_email(f"æ¯æ—¥ArXivè®ºæ–‡æŠ¥å‘Š - {user_name}", full_report, user_email))

        # ä¿å­˜æŠ¥å‘Šåˆ°ç”¨æˆ·ä¸“å±æ–‡ä»¶
        report_file = f"{user_dir}/report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(full_report)
        logger.success(f"ç”¨æˆ· {user_name} çš„æŠ¥å‘Šå·²å‘é€å¹¶ä¿å­˜åˆ° {report_file}")

def daily_job():
    """æ¯æ—¥ä»»åŠ¡ï¼šä¸ºæ‰€æœ‰é…ç½®çš„ç”¨æˆ·å¤„ç†è®ºæ–‡"""
    os.makedirs('temp', exist_ok=True)

    logger.info(f"å¼€å§‹æ¯æ—¥ä»»åŠ¡ï¼Œå…±æœ‰ {len(USERS_CONFIG)} ä¸ªç”¨æˆ·")

    for user_config in USERS_CONFIG:
        try:
            process_user(user_config)
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ· {user_config['name']} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    logger.success("æ‰€æœ‰ç”¨æˆ·å¤„ç†å®Œæˆ")

def run_scheduler():
    scheduler = BlockingScheduler()
    scheduler.add_job(
        daily_job, 
        trigger=CronTrigger(hour=16, minute=0),  # æ¯å¤©ä¸‹åˆ4ç‚¹è¿è¡Œ
        id='daily_arxiv_job',
        name='Daily ArXiv paper collection and summary'
    )
    
    logger.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯å¤©ä¸‹åˆ4:00è¿è¡Œ")
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")

if __name__ == "__main__":
    # é…ç½®loguru
    logger.add(
        "arxiv_pusher.log",
        rotation="10 MB",
        level="INFO",
        encoding="utf-8"
    )
    # å¦‚æœéœ€è¦ç«‹å³è¿è¡Œä¸€æ¬¡ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    daily_job()
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    run_scheduler()