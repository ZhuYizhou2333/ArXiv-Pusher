import os
import requests
from datetime import datetime, timedelta
from arxiv import Client, Search, SortCriterion, SortOrder
from PyPDF2 import PdfReader
import openai

from config import CONFIG, EMAIL_CONFIG

import smtplib
import socket
import asyncio
from email.mime.text import MIMEText
import markdown2  # å¯¼å…¥markdown2åº“
from loguru import logger
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
import time



async def send_email(subject, content):
    """å‘é€é‚®ä»¶é€šçŸ¥ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
    # å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML
    html_content = markdown2.markdown(content, extras=["tables", "mathjax", "fenced-code-blocks"])
    msg = MIMEText(html_content, "html", "utf-8")
    msg["Subject"] = subject
    msg["From"] = EMAIL_CONFIG["sender"]
    msg["To"] = EMAIL_CONFIG["receiver"]

    server = None
    try:
        logger.info("æ­£åœ¨è¿æ¥SMTPæœåŠ¡å™¨...")
        # å°†SMTPæ“ä½œæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        return await asyncio.get_event_loop().run_in_executor(
            None, lambda: _send_email_sync(msg, server)
        )
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False


def _send_email_sync(msg, server=None):
    """åŒæ­¥å‘é€é‚®ä»¶çš„å†…éƒ¨å‡½æ•°"""
    try:
        server = smtplib.SMTP(
            EMAIL_CONFIG["smtp_server"], EMAIL_CONFIG["smtp_port"], timeout=10
        )
        server.starttls()  # å¯ç”¨TLSåŠ å¯†
        server.login(EMAIL_CONFIG["sender"], EMAIL_CONFIG["password"])

        if EMAIL_CONFIG["receiver"].count(",") > 0:
            receivers = EMAIL_CONFIG["receiver"].split(",")
            server.sendmail(EMAIL_CONFIG["sender"], receivers, msg.as_string())
        else:
            server.sendmail(
                EMAIL_CONFIG["sender"], [EMAIL_CONFIG["receiver"]], msg.as_string()
            )

        logger.success("é‚®ä»¶å‘é€æˆåŠŸ")
        return True
    except socket.timeout:
        logger.warning("è¿æ¥SMTPæœåŠ¡å™¨è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡é‚®ä»¶å‘é€")
        return False
    except smtplib.SMTPException as e:
        logger.error(
            f"SMTPé”™è¯¯: {e.smtp_error.decode() if hasattr(e, 'smtp_error') else str(e)}"
        )
        return False
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False
    finally:
        if server:
            try:
                server.quit()
            except Exception as e:
                logger.warning(f"å…³é—­SMTPè¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


def fetch_papers():
    # ä»é…ç½®ä¸­è·å– arXiv åˆ†ç±»
    arxiv_categories = CONFIG["arxiv_categories"]
    # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ŒåªåŒ…å«é…ç½®ä¸­çš„ä¸»é¢˜
    search_query = " OR ".join([f"cat:{cat}" for cat in arxiv_categories])
    client = Client()  # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    search = Search(
        query=search_query,
        sort_by=SortCriterion.SubmittedDate,
        sort_order=SortOrder.Descending,
        max_results=100
    )
    
    papers = []
    cutoff_date = datetime.now() - timedelta(days=CONFIG["days_lookback"])
    
    for result in client.results(search):
        print(f"Processing paper: {result.title} published on {result.published}")
        if result.published.replace(tzinfo=None) > cutoff_date:
            papers.append({
                "title": result.title,
                "url": result.entry_id,
                "pdf_url": result.pdf_url,
                "abstract": result.summary,
                "authors": [a.name for a in result.authors],
                "published": result.published
            })
    print(f"Found {len(papers)} papers.")
    return papers

def download_pdf(url, filename):
    response = requests.get(url)
    with open(filename, 'wb') as f:
        f.write(response.content)

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as f:
        reader = PdfReader(f)
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

def gpt_summarize(text):
    prompt = f"""è¯·ä½ æ‹…ä»»é‡åŒ–é‡‘èé¢†åŸŸå­¦æœ¯è®ºæ–‡åŠ©ç†ï¼Œç”¨ä¸­æ–‡é’ˆå¯¹ä¸‹åˆ—è®ºæ–‡å†…å®¹è¿›è¡Œè¯¦ç»†æ€»ç»“ï¼Œå¹¶è¾“å‡ºä»¥ä¸‹ç»“æ„ï¼š
    1. ä¸­æ–‡ç¿»è¯‘ï¼šå¯¹è®ºæ–‡æ‘˜è¦è¿›è¡Œå‡†ç¡®ã€æµç•…çš„ä¸­æ–‡ç¿»è¯‘ï¼›
    2. åˆ›æ–°ç‚¹ï¼šåˆ—å‡º 3 ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼Œå¹¶è¯´æ˜å…¶é‡è¦æ€§ï¼›
    3. ç†è®ºä¸æ–¹æ³•ï¼šè¯¦ç»†æè¿°è®ºæ–‡é‡‡ç”¨çš„ä¸»è¦ç†è®ºæ¡†æ¶ï¼Œæ¨å¯¼æ­¥éª¤å’Œç ”ç©¶æ–¹æ³•ï¼Œå¿…è¦æ—¶å¯ä»¥ä½¿ç”¨ç®€å•çš„å…¬å¼ï¼›
    4. å®éªŒä¸ç»“æœï¼šæè¿°è®ºæ–‡é‡‡å–çš„æ ¸å¿ƒå®éªŒè®¾è®¡ã€æ•°æ®ç»“æœï¼Œå¯ä»¥ä½¿ç”¨è¡¨æ ¼ä»¥æ¸…æ™°åœ°å±•ç°ä¸åŒæ–¹æ³•çš„æ•ˆæœå¯¹æ¯”ï¼›
    5. ç»“è®ºä¸å½±å“ï¼šä»”ç»†åˆ—å‡ºè®ºæ–‡çš„æ‰€æœ‰çš„å…³é”®ç ”ç©¶ç»“è®ºï¼›
    6. ä¸»è¦å‚è€ƒæ–‡çŒ®ï¼šåˆ—ä¸¾å¹¶ç®€è¦ä»‹ç»è®ºæ–‡ä¸­å¼•ç”¨çš„ 2-3 ç¯‡æ ¸å¿ƒå‚è€ƒæ–‡çŒ®ï¼Œå¹¶æŒ‡å‡ºå…¶å¯¹æœ¬ç ”ç©¶çš„è´¡çŒ®ã€‚

    è®ºæ–‡åŸæ–‡å†…å®¹å¦‚ä¸‹ï¼š
    {text}

    è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œæ³¨æ„ä½¿ç”¨markdownæ ¼å¼è¿›è¡Œæ’ç‰ˆã€‚"""

    client = openai.OpenAI(
        base_url=CONFIG["base_url"],
        api_key=CONFIG["api_key"]
    )

    print(f"Requesting GPT to summarize: {text[:100]}...")
    print(f"Request length: {len(text)}")
    response = client.chat.completions.create(
        model=CONFIG["model"],
        messages=[{
            "role": "user",
            "content": prompt
        }],
    )
    print(f"Response: {response.choices[0].message.content[:100]}...")
    print(f"Response length: {len(response.choices[0].message.content)}")
    return response.choices[0].message.content

def daily_job():
    os.makedirs('temp', exist_ok=True)
    report = []
    papers = fetch_papers()
    
    for paper in papers:
        try:
            # ä¸‹è½½å¹¶å¤„ç†PDF
            pdf_path = f"temp/{paper['title']}.pdf"
            download_pdf(paper['pdf_url'], pdf_path)
            text = extract_text_from_pdf(pdf_path)

            # GPTæ€»ç»“
            summary = gpt_summarize(text)
            # os.remove(pdf_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶

            # æ„å»ºæŠ¥å‘Š
            report.append(f"""
## ğŸ“„è®ºæ–‡æ ‡é¢˜

{paper['title']}

## ğŸ“Š è®ºæ–‡ä¿¡æ¯
* **ä½œè€…**: {', '.join(paper['authors'])}
* **å‘è¡¨æ—¥æœŸ**: {paper['published'].strftime('%Y-%m-%d')}
* **é“¾æ¥**: [{paper['url']}]({paper['url']})

## ğŸ“ è®ºæ–‡æ€»ç»“
{summary}

{'â”€' * 80}
""")
        except Exception as e:
            print(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")
    
    if report:
        asyncio.run(send_email("æ¯æ—¥ArXivè®ºæ–‡æŠ¥å‘Š", '\n'.join(report)))
        with open('report.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        print("Reports sent.")

def run_scheduler():
    scheduler = BlockingScheduler()
    scheduler.add_job(
        daily_job, 
        trigger=CronTrigger(hour=16, minute=0),  # æ¯å¤©ä¸‹åˆ4ç‚¹è¿è¡Œ
        id='daily_arxiv_job',
        name='Daily ArXiv paper collection and summary'
    )
    
    logger.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯å¤©ä¸‹åˆ4:00è¿è¡Œ")
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")

if __name__ == "__main__":
    # å¦‚æœéœ€è¦ç«‹å³è¿è¡Œä¸€æ¬¡ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # daily_job()
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    run_scheduler()