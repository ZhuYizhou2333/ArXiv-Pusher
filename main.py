import os
import requests
from datetime import datetime, timedelta
from arxiv import Client, Search, SortCriterion, SortOrder
from PyPDF2 import PdfReader
import openai

from config import CONFIG, EMAIL_CONFIG

import smtplib
import socket
import asyncio
from email.mime.text import MIMEText
import markdown2  # å¯¼å…¥markdown2åº“
from loguru import logger
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.triggers.cron import CronTrigger
import time
from bs4 import BeautifulSoup
import subprocess



async def send_email(subject, content):
    """å‘é€é‚®ä»¶é€šçŸ¥ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
    # å°†Markdownå†…å®¹è½¬æ¢ä¸ºHTML
    html_content = markdown2.markdown(content, extras=["tables", "mathjax", "fenced-code-blocks"])
    msg = MIMEText(html_content, "html", "utf-8")
    msg["Subject"] = subject
    msg["From"] = EMAIL_CONFIG["sender"]
    msg["To"] = EMAIL_CONFIG["receiver"]

    server = None
    try:
        logger.info("æ­£åœ¨è¿æ¥SMTPæœåŠ¡å™¨...")
        # å°†SMTPæ“ä½œæ”¾åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        return await asyncio.get_event_loop().run_in_executor(
            None, lambda: _send_email_sync(msg, server)
        )
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False


def _send_email_sync(msg, server=None):
    """åŒæ­¥å‘é€é‚®ä»¶çš„å†…éƒ¨å‡½æ•°"""
    try:
        server = smtplib.SMTP(
            EMAIL_CONFIG["smtp_server"], EMAIL_CONFIG["smtp_port"], timeout=10
        )
        server.starttls()  # å¯ç”¨TLSåŠ å¯†
        server.login(EMAIL_CONFIG["sender"], EMAIL_CONFIG["password"])

        if EMAIL_CONFIG["receiver"].count(",") > 0:
            receivers = EMAIL_CONFIG["receiver"].split(",")
            server.sendmail(EMAIL_CONFIG["sender"], receivers, msg.as_string())
        else:
            server.sendmail(
                EMAIL_CONFIG["sender"], [EMAIL_CONFIG["receiver"]], msg.as_string()
            )

        logger.success("é‚®ä»¶å‘é€æˆåŠŸ")
        return True
    except socket.timeout:
        logger.warning("è¿æ¥SMTPæœåŠ¡å™¨è¶…æ—¶ï¼Œè·³è¿‡æœ¬æ¬¡é‚®ä»¶å‘é€")
        return False
    except smtplib.SMTPException as e:
        logger.error(
            f"SMTPé”™è¯¯: {e.smtp_error.decode() if hasattr(e, 'smtp_error') else str(e)}"
        )
        return False
    except Exception as e:
        logger.error(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return False
    finally:
        if server:
            try:
                server.quit()
            except Exception as e:
                logger.warning(f"å…³é—­SMTPè¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


def fetch_papers():
    # ä»é…ç½®ä¸­è·å– arXiv åˆ†ç±»
    arxiv_categories = CONFIG["arxiv_categories"]
    # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ŒåªåŒ…å«é…ç½®ä¸­çš„ä¸»é¢˜
    search_query = " OR ".join([f"cat:{cat}" for cat in arxiv_categories])
    client = Client()  # åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
    search = Search(
        query=search_query,
        sort_by=SortCriterion.SubmittedDate,
        sort_order=SortOrder.Descending,
        max_results=100
    )
    
    papers = []
    # Get the target date (exactly days_lookback days ago)
    target_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=CONFIG["days_lookback"])
    
    for result in client.results(search):
        print(f"Processing paper: {result.title} published on {result.published}")
        # Check if the paper was published on the target date
        published_dt = result.published.replace(tzinfo=None)
        if target_date <= published_dt :
            papers.append({
                "title": result.title,
                "url": result.entry_id,
                "pdf_url": result.pdf_url,
                "abstract": result.summary,
                "authors": [a.name for a in result.authors],
                "published": result.published,
                "categories": [c for c in result.categories],
                "primary_category": result.primary_category if result.primary_category else None
            })
    print(f"Found {len(papers)} papers published from {target_date.strftime('%Y-%m-%d')}")
    return papers

def download_pdf(url, filename, max_retries=3):
    """ä¸‹è½½PDFæ–‡ä»¶ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""
        # ç¡®ä¿URLæ˜¯æ­£ç¡®çš„PDFé“¾æ¥
    if 'arxiv.org' in url and not url.endswith('.pdf'):
        # ä»URLæå–è®ºæ–‡ID
        paper_id = url.split('/')[-1]
        url = f"https://arxiv.org/pdf/{paper_id}.pdf"
    
    print(f"å°è¯•ä¸‹è½½: {url}")
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)  # æ·»åŠ è¶…æ—¶å‚æ•°
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸä¸”å†…å®¹ç±»å‹æ˜¯PDF
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '')
                if 'pdf' not in content_type.lower() and len(response.content) < 10000:
                    print(f"è­¦å‘Š: å“åº”å¯èƒ½ä¸æ˜¯PDFæ–‡ä»¶ (Content-Type: {content_type})")
                
                with open(filename, 'wb') as f:
                    f.write(response.content)
                
                # éªŒè¯æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(filename)
                if file_size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
                    print(f"è­¦å‘Š: ä¸‹è½½çš„æ–‡ä»¶è¿‡å° ({file_size} å­—èŠ‚)")
                    continue
                
                return True
            else:
                print(f"ä¸‹è½½å¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
        except Exception as e:
            print(f"å°è¯• {attempt+1}/{max_retries} å¤±è´¥: {str(e)}")
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
        if attempt < max_retries - 1:
            time.sleep(2 * (attempt + 1))  # æŒ‡æ•°é€€é¿
    
    return False

def extract_text_from_pdf(pdf_path, paper):
    """ä»PDFæå–æ–‡æœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    text = ""
    try:
        with open(pdf_path, 'rb') as f:
            try:
                reader = PdfReader(f)
                for page_num, page in enumerate(reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except Exception as e:
                        print(f"æ— æ³•æå–ç¬¬ {page_num+1} é¡µ: {str(e)}")
            except Exception as e:
                print(f"PDFè§£æå¤±è´¥: {str(e)}")
                # å¦‚æœæ˜¯EOFé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨å¦ä¸€ç§æ–¹æ³•
                if "EOF" in str(e):
                    # å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–åº“å¦‚pdfmineræˆ–pdfplumber
                    print("å°è¯•å¤‡ç”¨PDFè§£ææ–¹æ³•")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ å¤‡ç”¨è§£æä»£ç 
    except Exception as e:
        print(f"æ— æ³•æ‰“å¼€PDFæ–‡ä»¶: {str(e)}")
    
    return text

def download_pdf_and_extract_text(paper):
    """ä¸‹è½½PDFå¹¶æå–æ–‡æœ¬ï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    pdf_path = f"temp/{paper['title']}.pdf"
    if download_pdf(paper['pdf_url'], pdf_path):
        text = extract_text_from_pdf(pdf_path, paper)
        if not text:
            print(f"è­¦å‘Š: æ— æ³•ä» {paper['title']} æå–æ–‡æœ¬")
        return text
    else:
        print(f"é”™è¯¯: æ— æ³•ä¸‹è½½ {paper['title']} çš„PDF")
        return ""

def download_html_and_extract_text(paper):
    """ä»arxivä¸‹è½½HTMLç‰ˆæœ¬ï¼Œä¿å­˜ä¸ºPDFï¼Œç„¶åæå–æ–‡æœ¬"""
    try:
        # ä»paper URLç”ŸæˆHTMLé“¾æ¥
        url = paper['url']
        if 'arxiv.org' in url:
            paper_id = url.split('/')[-1]
            html_url = f"https://arxiv.org/html/{paper_id}"
        else:
            html_url = url.replace('.pdf', '.html')
        
        print(f"å°è¯•ä¸‹è½½HTML: {html_url}")
        
        # ä¸‹è½½HTMLå†…å®¹
        response = requests.get(html_url, timeout=30)
        
        if response.status_code == 200:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶HTMLæ–‡ä»¶
            temp_html_path = f"temp/{paper['title']}_temp.html"
            with open(temp_html_path, 'wb') as f:
                f.write(response.content)
            
            # ä½¿ç”¨wkhtmltopdfå°†HTMLè½¬æ¢ä¸ºPDF (éœ€è¦å®‰è£…wkhtmltopdf)
            pdf_path = f"temp/{paper['title']}_from_html.pdf"
            try:
                subprocess.run(['wkhtmltopdf', temp_html_path, pdf_path], 
                              check=True, timeout=60)
                print(f"å·²å°†HTMLè½¬æ¢ä¸ºPDF: {pdf_path}")
                
                # å°è¯•ä»ç”Ÿæˆçš„PDFæå–æ–‡æœ¬
                pdf_text = extract_text_from_pdf(pdf_path, paper)
                if pdf_text and len(pdf_text) > 1000:
                    return pdf_text
            except Exception as pdf_err:
                print(f"HTMLè½¬PDFå¤±è´¥: {str(pdf_err)}")
            
            # å¦‚æœPDFè½¬æ¢å¤±è´¥æˆ–æå–æ–‡æœ¬ä¸è¶³ï¼Œåˆ™ç›´æ¥ä»HTMLæå–
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.extract()
                
            # è·å–æ–‡æœ¬
            text = soup.get_text(separator="\n", strip=True)
            
            # å¤„ç†ç©ºç™½å­—ç¬¦
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            print(f"ä»HTMLæå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
            return text
        else:
            print(f"HTMLä¸‹è½½å¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
            return ""
    except Exception as e:
        print(f"HTMLå¤„ç†é”™è¯¯: {str(e)}")
        return ""

def get_paper_text(paper):
    """å°è¯•å¤šç§æ–¹å¼è·å–è®ºæ–‡æ–‡æœ¬å†…å®¹"""
    # é¦–å…ˆå°è¯•PDFæ–¹å¼
    text = download_pdf_and_extract_text(paper)
    
    # å¦‚æœPDFæ–¹å¼å¤±è´¥ï¼Œå°è¯•HTMLæ–¹å¼
    if not text or len(text) < 1000:  # å†…å®¹å¤ªå°‘å¯èƒ½æ˜¯æå–å¤±è´¥
        print(f"PDFæå–å¤±è´¥æˆ–å†…å®¹å¤ªå°‘ï¼Œå°è¯•HTMLæ–¹å¼")
        text = download_html_and_extract_text(paper)

    # å¦‚æœtexté•¿äº129024 åˆ™æˆªæ–­
    if len(text) > 129024:
        print(f"æ–‡æœ¬å†…å®¹è¿‡é•¿ï¼Œæˆªæ–­åˆ°å‰129024å­—ç¬¦")
        text = text[:129024]
    if not text:
        text = paper['abstract']  # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ‘˜è¦ä½œä¸ºæœ€åçš„fallback
    
    return text

def gpt_summarize(text):
    prompt = f"""è¯·ä½ æ‹…ä»»é‡åŒ–é‡‘èé¢†åŸŸå­¦æœ¯è®ºæ–‡åŠ©ç†ï¼Œç”¨ä¸­æ–‡é’ˆå¯¹ä¸‹åˆ—è®ºæ–‡å†…å®¹è¿›è¡Œè¯¦ç»†æ€»ç»“ï¼Œå¹¶è¾“å‡ºä»¥ä¸‹ç»“æ„ï¼š
    1. ä¸­æ–‡ç¿»è¯‘ï¼šå¯¹è®ºæ–‡æ‘˜è¦è¿›è¡Œå‡†ç¡®ã€æµç•…çš„ä¸­æ–‡ç¿»è¯‘ï¼›
    2. åˆ›æ–°ç‚¹ï¼šåˆ—å‡º 3 ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼Œå¹¶è¯´æ˜å…¶é‡è¦æ€§ï¼›
    3. ç†è®ºä¸æ–¹æ³•ï¼šè¯¦ç»†æè¿°è®ºæ–‡é‡‡ç”¨çš„ä¸»è¦ç†è®ºæ¡†æ¶ï¼Œæ¨å¯¼æ­¥éª¤å’Œç ”ç©¶æ–¹æ³•ï¼Œå¿…è¦æ—¶å¯ä»¥ä½¿ç”¨ç®€å•çš„å…¬å¼ï¼›
    4. å®éªŒä¸ç»“æœï¼šæè¿°è®ºæ–‡é‡‡å–çš„æ ¸å¿ƒå®éªŒè®¾è®¡ã€æ•°æ®ç»“æœï¼Œå¯ä»¥ä½¿ç”¨è¡¨æ ¼ä»¥æ¸…æ™°åœ°å±•ç°ä¸åŒæ–¹æ³•çš„æ•ˆæœå¯¹æ¯”ï¼›
    5. ç»“è®ºä¸å½±å“ï¼šä»”ç»†åˆ—å‡ºè®ºæ–‡çš„æ‰€æœ‰çš„å…³é”®ç ”ç©¶ç»“è®ºï¼›
    6. ä¸»è¦å‚è€ƒæ–‡çŒ®ï¼šåˆ—ä¸¾å¹¶ç®€è¦ä»‹ç»è®ºæ–‡ä¸­å¼•ç”¨çš„ 2-3 ç¯‡æ ¸å¿ƒå‚è€ƒæ–‡çŒ®ï¼Œå¹¶æŒ‡å‡ºå…¶å¯¹æœ¬ç ”ç©¶çš„è´¡çŒ®ã€‚

    è®ºæ–‡åŸæ–‡å†…å®¹å¦‚ä¸‹ï¼š
    {text}

    è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œæ³¨æ„ä½¿ç”¨markdownæ ¼å¼è¿›è¡Œæ’ç‰ˆï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ ‡è®°ã€‚"""

    client = openai.OpenAI(
        base_url=CONFIG["base_url"],
        api_key=CONFIG["api_key"]
    )

    print(f"Requesting GPT to summarize: {text[:100]}...")
    print(f"Request length: {len(text)}")
    response = client.chat.completions.create(
        model=CONFIG["model"],
        messages=[{
            "role": "user",
            "content": prompt
        }],
    )
    print(f"Response: {response.choices[0].message.content[:100]}...")
    print(f"Response length: {len(response.choices[0].message.content)}")
    return response.choices[0].message.content

def daily_job():
    os.makedirs('temp', exist_ok=True)
    report = []
    papers = fetch_papers()
    
    for paper in papers:
        try:
            # ä¸‹è½½å¹¶å¤„ç†PDF
            text = get_paper_text(paper)

            # GPTæ€»ç»“
            summary = gpt_summarize(text)
            # os.remove(pdf_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶

            # æ„å»ºæŠ¥å‘Š
            report.append(f"""
## ğŸ“„è®ºæ–‡æ ‡é¢˜

{paper['title']}

## ğŸ“Š è®ºæ–‡ä¿¡æ¯
* **ä½œè€…**: {', '.join(paper['authors'])}
* **å‘è¡¨æ—¥æœŸ**: {paper['published'].strftime('%Y-%m-%d')}
* **é“¾æ¥**: [{paper['url']}]({paper['url']})
* **ä¸»è¦åˆ†ç±»**: {paper["primary_category"] if "primary_category" in paper else "æœªçŸ¥åˆ†ç±»"}
* **æ‰€å±åˆ†ç±»**: {paper["categories"] if "categories" in paper else "æœªçŸ¥åˆ†ç±»"}


## ğŸ“ è®ºæ–‡æ€»ç»“
{summary}

{'â”€' * 80}
""")
        except Exception as e:
            print(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")
            # å‘é€é‚®ä»¶é€šçŸ¥å¤„ç†å¤±è´¥
            report.append(f"å¤„ç†è®ºæ–‡å¤±è´¥: {paper['title']}ï¼Œé”™è¯¯: {str(e)}")
    
    if report:
        asyncio.run(send_email("æ¯æ—¥ArXivè®ºæ–‡æŠ¥å‘Š", '\n'.join(report)))
        with open('report.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        print("Reports sent.")

def run_scheduler():
    scheduler = BlockingScheduler()
    scheduler.add_job(
        daily_job, 
        trigger=CronTrigger(hour=16, minute=0),  # æ¯å¤©ä¸‹åˆ4ç‚¹è¿è¡Œ
        id='daily_arxiv_job',
        name='Daily ArXiv paper collection and summary'
    )
    
    logger.info("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯å¤©ä¸‹åˆ4:00è¿è¡Œ")
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")

if __name__ == "__main__":
    # å¦‚æœéœ€è¦ç«‹å³è¿è¡Œä¸€æ¬¡ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    daily_job()
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    run_scheduler()